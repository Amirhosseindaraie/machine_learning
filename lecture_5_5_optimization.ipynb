{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_5.5_optimization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fbeilstein/machine_learning/blob/master/lecture_5_5_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3obJiLBgLSDR",
        "colab_type": "text"
      },
      "source": [
        "#Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULbc5LMHSCQ",
        "colab_type": "text"
      },
      "source": [
        "Suppose\n",
        "\n",
        "$$\n",
        "f: X \\rightarrow \\mathbb{R}\n",
        "$$\n",
        "\n",
        "where $X \\subset \\mathbb{R}^n$.\n",
        "\n",
        "Taylor exapansion\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol {x}+ \\boldsymbol {h})=f(\\boldsymbol {x})+\\nabla f(\\boldsymbol {x})\\boldsymbol {h} +\\boldsymbol {h}^{\\top} \\boldsymbol {H} \\boldsymbol {h}+o(||{\\boldsymbol {h}||}^3).\n",
        "$$\n",
        "\n",
        "$\\nabla f(\\boldsymbol {x})$ is the gradient of the function\n",
        "\n",
        "$$\n",
        "\\nabla_i f(\\boldsymbol {x})=\\frac{\\partial f}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "The gradient of $f$ at a point is a vector pointing in the direction of the steepest slope or grade at that point. The steepness of the slope at that point is given by the magnitude of the gradient vector.\n",
        "$\\boldsymbol {H}_{ij}$ is the Hessian - a square matrix of second-order partial derivatives\n",
        "\n",
        "$$\n",
        "\\boldsymbol {H}_{ij} f(\\boldsymbol {x})=\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n",
        "$$\n",
        "\n",
        "$\\boldsymbol {H}_{ij}$ is symmetric.\n",
        "\n",
        "If function has extremum at point $\\boldsymbol {x}_0$\n",
        "\n",
        "$$\n",
        "\\nabla f(\\boldsymbol {x}_0)=0\n",
        "$$\n",
        "\n",
        "Convex functions\n",
        "\n",
        "$$\n",
        "\\forall  \\boldsymbol {x}_1,\\boldsymbol {x}_2 \\in X, \\quad \\forall t\\in [0,1] :\\quad f\\left(t\\boldsymbol {x}_1+(1-t)\\boldsymbol {x}_2\\right)\\leq tf(\\boldsymbol {x}_1)+(1-t)f(\\boldsymbol {x}_2)\n",
        "$$\n",
        "\n",
        "Any local minimum of a convex function is also a global minimum. A strictly convex function will have at most one global minimum.\n",
        "\n",
        "Convex set or a convex region is a subset of a Euclidean space, or more generally an affine space over the reals, that intersects every line into a line segment (possibly empty).\n",
        "\n",
        "Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tQ5M5TG9ZGM",
        "colab_type": "text"
      },
      "source": [
        "#Standard form of optimization problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0KRTV0M9eEk",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "& \\text{minimize}_\\boldsymbol{x} \\, f(\\boldsymbol{x}) \\\\\n",
        "& \\text{subject to } \\qquad\n",
        "\\begin{aligned} \n",
        "& g_i(\\boldsymbol{x}) \\leq 0 \\qquad i=1\\cdots m \\\\\n",
        "& h_j(\\boldsymbol{x}) = 0 \\qquad j=1\\cdots p \\\\\n",
        "\\end{aligned}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{x} \\in \\mathbb{R}^n$ is the optimization variable, the functions $f,g_1, \\cdots,g_m$ are convex and the functions $h_1, \\cdots, h_p$ are affine.\n",
        "In this notation, the function $f$ is the objective function of the problem, and the functions $g_{i}$ and $h_{i}$ are referred to as the constraint functions.\n",
        "\n",
        "The function f is called, variously, an **objective function**, a **loss function** or **cost (regret) function** (in economy) or an **energy function** or e**nergy functional** (in physics). \n",
        "If we want to find maximum of a function it's usually cold a **utility function** or **fitness function**. \n",
        "\n",
        "The following are useful properties of convex optimization problems:\n",
        "* every local minimum is a global minimum;\n",
        "*  the optimal set is convex;\n",
        "*  if the objective function is strictly convex, then the problem has at most one optimal point.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAsF3ehFZsdt",
        "colab_type": "text"
      },
      "source": [
        "#Test functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmj4YIyyYSa7",
        "colab_type": "text"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Test_functions_for_optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuhT8u2_cVbP",
        "colab_type": "text"
      },
      "source": [
        "#Iterative methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzmKFbxzbxqx",
        "colab_type": "text"
      },
      "source": [
        "The iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values.\n",
        "\n",
        "Methods that evaluate Hessians (or approximate Hessians, using finite differences):\n",
        "* Newton's method\n",
        "* Sequential quadratic programming: A Newton-based method for small-medium scale constrained problems. Some versions can handle large-dimensional problems.\n",
        "* Interior point methods: This is a large class of methods for constrained optimization. Some interior-point methods use only (sub)gradient information and others of which require the evaluation of Hessians.\n",
        "\n",
        "Methods that evaluate gradients, or approximate gradients in some way (or even subgradients):\n",
        "* Coordinate descent methods: Algorithms which update a single coordinate in each iteration\n",
        "* Conjugate gradient methods: Iterative methods for large problems. (In theory, these methods terminate in a finite number of steps with quadratic objective functions, but this finite termination is not observed in practice on finite–precision computers.)\n",
        "* Gradient descent (alternatively, \"steepest descent\" or \"steepest ascent\"): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems.\n",
        "* Subgradient methods - An iterative method for large locally Lipschitz functions using generalized gradients. Following Boris T. Polyak, subgradient–projection methods are similar to conjugate–gradient methods.\n",
        "* Bundle method of descent: An iterative method for small–medium-sized problems with locally Lipschitz functions, particularly for convex minimization problems. (Similar to conjugate gradient methods)\n",
        "* Ellipsoid method: An iterative method for small problems with quasiconvex objective functions and of great theoretical interest, particularly in establishing the polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.\n",
        "* Conditional gradient method (Frank–Wolfe) for approximate minimization of specially structured problems with linear constraints, especially with traffic networks. For general unconstrained problems, this method reduces to the gradient method, which is regarded as obsolete (for almost all problems).\n",
        "* Quasi-Newton methods: Iterative methods for medium-large problems (e.g. N<1000).\n",
        "* Simultaneous perturbation stochastic approximation (SPSA) method for stochastic optimization; uses random (efficient) gradient approximation.\n",
        "\n",
        "Methods that evaluate only function values: If a problem is continuously differentiable, then gradients can be approximated using finite differences, in which case a gradient-based method can be used.\n",
        "* Interpolation methods\n",
        "* Pattern search methods, which have better convergence properties than the Nelder–Mead heuristic (with simplices), which is listed below.\n",
        "\n",
        "Besides (finitely terminating) algorithms and (convergent) iterative methods, there are heuristics. A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcgUG6vAJ8Dj",
        "colab_type": "text"
      },
      "source": [
        "#Steepest descent methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k3NhnRHxSwC",
        "colab_type": "text"
      },
      "source": [
        "**Standard gradient descent**\n",
        "$$\n",
        "\\boldsymbol{x}_{n+1}=\\boldsymbol{x}_{n}-\\alpha \\cdot \\nabla L(\\boldsymbol{x}_{n})\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is called learning rate. The problem with gradient descent is that it's not adaptive: $\\alpha$ is chosen once and for all direction of movement $\\nabla L(\\boldsymbol{x}_{n})$ doesn't take into account the history.\n",
        "\n",
        "| Optimizer | Year | Learning rate $\\alpha$ | Gradient $\\nabla L$\n",
        "|----|---|---|---\n",
        "Standard  GD|1847 | ||\n",
        "Momentum|1964| |               $\\checkmark$ |\n",
        "AdaGrad |2011| $\\checkmark$ |               |\n",
        "RMSprop |2011| $\\checkmark$ |               |\n",
        "Adadelta|2012| $\\checkmark$ |               |\n",
        "Nesterov|2013| |               $\\checkmark$ |\n",
        "Adam    |2014| $\\checkmark$ | $\\checkmark$| \n",
        "AdaMax  |2015| $\\checkmark$ | $\\checkmark$|\n",
        "Nadam   |2015| $\\checkmark$ | $\\checkmark$|\n",
        "AMSGrad |2018| $\\checkmark$ | $\\checkmark$|\n",
        "\n",
        "Here is a glossary \n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{AdaGrad} &= \\text{adaptive gradient} \\\\\n",
        "\\text{RMSProp} &= \\text{root mean square propagation} \\\\\n",
        "\\text{Adam} &= \\text{adaptive movement} \\\\\n",
        "\\text{Nadam} &= \\text{Nesterov + adam} \\\\\n",
        "\\text{Adadelta} &= \\text{adaptive delta} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Momentum**\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}_{n+1}&=\\boldsymbol{x}_{n}-\\alpha \\boldsymbol{V}_{n} \\\\\n",
        "\\boldsymbol{V}_{n}&=\\beta \\, \\boldsymbol{V}_{n-1}+(1-\\beta)\\nabla L(\\boldsymbol{x}_{n})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Instead of depending only on the current gradient to update the weight, gradient descent with momentum (Polyak, 1964) replaces the current gradient with $V$ (which stands for velocity), the exponential moving average of current and past gradients.\n",
        "\n",
        "[Here](https://distill.pub/2017/momentum/) is a nice interactive article that explains why momentum method was a major breakthrough.\n",
        "\n",
        "\n",
        "**Nesterov**\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}_{n+1}&=\\boldsymbol{x}_{n}-\\alpha \\boldsymbol{V}_{n} \\\\\n",
        "\\boldsymbol{V}_{n}&=\\beta \\, \\boldsymbol{V}_{n-1}+(1-\\beta)\\nabla L(\\boldsymbol{x}^*) \\\\\n",
        "\\boldsymbol{x}^*&=\\boldsymbol{x}_{n}-\\alpha \\boldsymbol{V}_{n-1}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Adagrad**\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}_{n+1}&=\\boldsymbol{x}_{n}-\\frac{\\alpha}{\\sqrt{S_n+\\epsilon}}\\cdot \\nabla L(\\boldsymbol{x}_{n}) \\\\\n",
        "S_n&=S_{n-1}+\\left(\\nabla L(\\boldsymbol{x}_{n})\\right)^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Adaptive gradient, or AdaGrad (Duchi et al., 2011), works on the learning rate component by dividing the learning rate by the square root of S, which is the cumulative sum of current and past squared gradients.\n",
        "\n",
        "**RMSprop**\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}_{n+1}&=\\boldsymbol{x}_{n}-\\frac{\\alpha}{\\sqrt{S_n+\\epsilon}}\\cdot \\nabla L(\\boldsymbol{x}_{n}) \\\\\n",
        "S_n&=\\beta \\, S_{n-1}+(1-\\beta)\\left(\\nabla L(\\boldsymbol{x}_{n}) \\right)^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Adadelta**\n",
        "\n",
        "$$\\cdots$$\n",
        "\n",
        "**Adam**\n",
        "\n",
        "$$\\cdots$$\n",
        "\n",
        "**AdaMax**\n",
        "$$\\cdots$$\n",
        "\n",
        "**Nadam**\n",
        "\n",
        "$$\\cdots$$\n",
        "\n",
        "**AMSGrad**\n",
        "$$\\cdots$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQYNeK1OsY_",
        "colab_type": "text"
      },
      "source": [
        "#Conjugate gradient method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Yb-cWMPcC2",
        "colab_type": "text"
      },
      "source": [
        " #truncated-Newton and quasi-Newton algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7xAQzbsA9To",
        "colab_type": "text"
      },
      "source": [
        "#Nelder–Mead method (Simplex method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-LibEWBDhj",
        "colab_type": "text"
      },
      "source": [
        "#Powell's method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AW9W0VWBGoq",
        "colab_type": "text"
      },
      "source": [
        "#BFGS method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPwXjDMwBUoR",
        "colab_type": "text"
      },
      "source": [
        "#L-BFGS-B method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYYxvVB1BQU9",
        "colab_type": "text"
      },
      "source": [
        "#Newton CG method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAApvg12Bb7V",
        "colab_type": "text"
      },
      "source": [
        "#TNC method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU0dhROMBe42",
        "colab_type": "text"
      },
      "source": [
        "#SLSQP method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJJCOkWdB05n",
        "colab_type": "text"
      },
      "source": [
        "#COBYLA method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMNNQl1VCaUk",
        "colab_type": "text"
      },
      "source": [
        "#Trust region methods"
      ]
    }
  ]
}