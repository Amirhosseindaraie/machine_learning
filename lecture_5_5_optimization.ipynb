{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_5.5_optimization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fbeilstein/machine_learning/blob/master/lecture_5_5_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3obJiLBgLSDR",
        "colab_type": "text"
      },
      "source": [
        "#Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NULbc5LMHSCQ",
        "colab_type": "text"
      },
      "source": [
        "Suppose\n",
        "\n",
        "$$\n",
        "f: X \\rightarrow \\mathbb{R}\n",
        "$$\n",
        "\n",
        "where $X \\subset \\mathbb{R}^n$.\n",
        "\n",
        "Taylor exapansion\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol {x}+ \\boldsymbol {h})=f(\\boldsymbol {x})+\\nabla f(\\boldsymbol {x})\\boldsymbol {h} +\\boldsymbol {h}^{\\top} \\boldsymbol {H} \\boldsymbol {h}+o(||{\\boldsymbol {h}||}^3).\n",
        "$$\n",
        "\n",
        "$\\nabla f(\\boldsymbol {x})$ is the gradient of the function\n",
        "\n",
        "$$\n",
        "\\nabla_i f(\\boldsymbol {x})=\\frac{\\partial f}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "$\\boldsymbol {H}_{ij}$ is the Hessian - a square matrix of second-order partial derivatives\n",
        "\n",
        "$$\n",
        "\\boldsymbol {H}_{ij} f(\\boldsymbol {x})=\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n",
        "$$\n",
        "\n",
        "The gradient of $f$ at a point is a vector pointing in the direction of the steepest slope or grade at that point. The steepness of the slope at that point is given by the magnitude of the gradient vecto\n",
        "\n",
        "If function has extremum at point $\\boldsymbol {x}_0$\n",
        "\n",
        "$$\n",
        "\\nabla f(\\boldsymbol {x}_0)=0\n",
        "$$\n",
        "\n",
        "Convex functions\n",
        "\n",
        "$$\n",
        "\\forall  \\boldsymbol {x}_1,\\boldsymbol {x}_2 \\in X, \\quad \\forall t\\in [0,1] :\\quad f\\left(t\\boldsymbol {x}_1+(1-t)\\boldsymbol {x}_2\\right)\\leq tf(\\boldsymbol {x}_1)+(1-t)f(\\boldsymbol {x}_2)\n",
        "$$\n",
        "\n",
        "Any local minimum of a convex function is also a global minimum. A strictly convex function will have at most one global minimum.\n",
        "\n",
        "Convex set or a convex region is a subset of a Euclidean space, or more generally an affine space over the reals, that intersects every line into a line segment (possibly empty).\n",
        "\n",
        "Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. "
      ]
    }
  ]
}