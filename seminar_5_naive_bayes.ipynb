{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_4.5_statistics.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fbeilstein/machine_learning/blob/master/seminar_5_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DP7FJypsTId",
        "colab_type": "text"
      },
      "source": [
        "#Road map for Naive Bayes text classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Z71lcwrnT_",
        "colab_type": "text"
      },
      "source": [
        "We are about to design our own Naive Bayes text classifier. For thay we need to write $6$ classes\n",
        "* `TagsEstimator`\n",
        "* `Vectorizer`\n",
        "* `DocWordCounter`\n",
        "* `ModelParametersEstimator`\n",
        "* `PosteriorCalculator`\n",
        "* `DecisionRule`\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD6O72hfJaYj",
        "colab_type": "text"
      },
      "source": [
        "#Estimating tags prior probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUgepi7XIr-T",
        "colab_type": "text"
      },
      "source": [
        "Suppose in a training set there are 15 documents tagged $0$ ($C[0]=\"Physics\"$), 25 documents tagged $1$ (label $C[1]=\"Economics\"$) and 30 docs with tag $2$ ($C[2]=\"Religion\"$).\n",
        "\n",
        "Estimate $P(k)$ using MLE (emphirical frequencies)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQGCETyiIWy",
        "colab_type": "text"
      },
      "source": [
        "You are also given a list of tags $tags$.\n",
        "\n",
        "Write function that calculates $P(k)$.\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb_00.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Kx6lw-kjn4",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhfIJLQyi_jg",
        "colab_type": "code",
        "outputId": "e30f7851-4f66-4c23-a703-7a6ec50b5314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "class tagsEstimator:\n",
        "  def calculatePriors(self, p):\n",
        "    counter = Counter(p)\n",
        "    out=[]\n",
        "    for i in counter:\n",
        "      out.append(counter[i]/len(p))\n",
        "    return out\n",
        "\n",
        "TE = tagsEstimator()\n",
        "TE.calculatePriors([\"Physics\", \"Lyrics\", \"Lyrics\", \"Lyrics\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25, 0.75]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z9XtqJziszO",
        "colab_type": "text"
      },
      "source": [
        "#Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHC8s5RuivSY",
        "colab_type": "text"
      },
      "source": [
        "Write your own simple Vectorizer that takes a vector $corpus$ of strings, prepares $vocabulary$ and returns a table $|corpus|\\times|vocabulary|$ with counts of words of the vocabulary in each document. \n",
        "Your class would be analog of ```sklearn.feature_extraction.text.CountVectorizer```.\n",
        "The interface of our class should be as follows:\n",
        "* ```fit_transform(self, corpus)``` - feeds vectorizer a list of strings.\n",
        "* ```get_feature_names(self)``` - returns of lexigraphically sorted lowercase words in the vocabulary.\n",
        "* ```toarray(self)``` - returns array $|corpus|\\times|vocabulary|$ with counts.\n",
        "\n",
        "Note: you should get rid of punctuation marks and convert your lines into lowercase in all your documents.\n",
        "\n",
        "Now modify your code so that the counts are boolean ($1$ if the word is present in the document and $0$ otherwise).\n",
        "This corresponds to ```CountVectorizer(binary = True)```.\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb_0.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w7zx6c56TGd",
        "colab_type": "text"
      },
      "source": [
        "##Standard Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv8taV1Ejqku",
        "colab_type": "code",
        "outputId": "69f1a8bd-cd8d-4b84-f079-8d3488b8e165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['This is the first document.', 'This document is the second document.',\n",
        "'And this is the third one.', 'Is this the first document?']\n",
        "\n",
        "vectorizer_C = CountVectorizer()\n",
        "C = vectorizer_C.fit_transform(corpus)\n",
        "print(\"Vocabulary : \", vectorizer_C.get_feature_names())\n",
        "print(\"arr :\", C.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary :  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "arr : [[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI1XIj3Wzz8B",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4o_DKn_kUHE",
        "colab_type": "code",
        "outputId": "3075c613-28b1-48df-f4f2-181b12c40c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "class SimpleCountVectorizer:\n",
        "  \"\"\"Vectorizer that counts number of word occurrences in documents\"\"\"\n",
        "  def __init__(self, binary=False):\n",
        "    self.binary = binary\n",
        "    \n",
        "  def fit_transform(self, corpus):\n",
        "    self.vocabulary = set()\n",
        "    counters = []\n",
        "    self.counts = []\n",
        "    \n",
        "    for d in corpus:\n",
        "      counter = Counter()\n",
        "      table = str.maketrans({key: None for key in string.punctuation})\n",
        "      d_without_punct = d.translate(table) # deletes all punctuation\n",
        "      words = d_without_punct.lower().split()\n",
        "      self.vocabulary.update(set(words))\n",
        "      counter |= Counter(words)\n",
        "      counters.append(counter)\n",
        "    self.vocabulary = sorted(self.vocabulary)  \n",
        "    \n",
        "    for d_index in range(0, len(corpus)):\n",
        "      counts = []\n",
        "      for word in self.vocabulary:\n",
        "        count = int(counters[d_index][word] > 0) if self.binary else counters[d_index][word]\n",
        "        counts.append(count)\n",
        "      self.counts.append(counts)\n",
        " \n",
        "  def get_feature_names(self):\n",
        "    return self.vocabulary\n",
        "  \n",
        "  def toarray(self):\n",
        "    return  self.counts\n",
        "      \n",
        "  \n",
        "corpus = ['This is the first document.', 'This document is the second document.',\n",
        "'And this is the third one.', 'Is this the first document?']\n",
        "SCV = SimpleCountVectorizer(binary=True)\n",
        "SCV.fit_transform(corpus)\n",
        "print(\"Vocabulary : \", SCV.get_feature_names())\n",
        "print(\"arr :\", SCV.toarray())\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary :  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "arr : [[0, 1, 1, 1, 0, 0, 1, 0, 1], [0, 1, 0, 1, 0, 1, 1, 0, 1], [1, 0, 0, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 0, 0, 1, 0, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4SpnHwC5D3M",
        "colab_type": "text"
      },
      "source": [
        "#Document Word counter\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLG4mZFXh39S",
        "colab_type": "text"
      },
      "source": [
        "Given vocabulary and a document you can calcuate word counts for all words of vocabulary in the document.\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb_1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvFUwxSGlZeB",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1aSs6BinUjH",
        "colab_type": "code",
        "outputId": "9e371f77-2003-4be9-877f-3f7b2346b22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  \n",
        "  class docWordCounter:\n",
        "    def count(self, vocabulary, document):\n",
        "      counts = []\n",
        "      table = str.maketrans({key: None for key in string.punctuation})\n",
        "      d_without_punct = document.translate(table) # deletes all punctuation\n",
        "      words = d_without_punct.lower().split()\n",
        "      for i in vocabulary:\n",
        "        counts.append(words.count(i))\n",
        "      return counts\n",
        "  \n",
        "WC = wordCounter()\n",
        "WC.count([\"s\",\"d\"],\"s s d hjhkj\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-h7y_gHjYER",
        "colab_type": "text"
      },
      "source": [
        "#Parameters estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2b-Zv350DTN",
        "colab_type": "text"
      },
      "source": [
        "## By hand multinomial model parameters estimation for $1$ tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RToFZMcynz4",
        "colab_type": "text"
      },
      "source": [
        "Suppose your dictionary contains words $\\{\"position\", \"velocity\", \"stocks\"\\}$.\n",
        "Suppose you have $3$ documents tagged as 1 (label $\"Physics\"$).\n",
        "\n",
        "Your 1st document contains \n",
        "\n",
        "* word \"position\" $x_1=0$ times\n",
        "* word \"velocity\" $x_2=1$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "Your 2nd document contains \n",
        "\n",
        "* word \"position\" $x_1=0$ times\n",
        "* word \"velocity\" $x_2=1$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "Your 3rd document contains \n",
        "\n",
        "* word \"position\" $x_1=3$ times\n",
        "* word \"velocity\" $x_2=0$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "Suppose you adopt multinomial model for word occurence\n",
        "\n",
        "$$\n",
        "P(\\{x_1,x_2,x_3\\}|\"Phycics\")=\\frac{(x_1+x_2+x_3)!}{x_1!x_2!x_3!} p_{1}^{x_1}p_{2}^{x_2}p_{3}^{x_3}\n",
        "$$\n",
        "\n",
        "With given data use emphirical frequency (with smoothing $\\alpha=0.1$) to estimate model parameters $p_1,p_2,p_3$ and verify that $p_1+p_2+p_3=1$.\n",
        "\n",
        "Note: You can aggregate counts in our three documents to get \n",
        "\n",
        "* word \"position\" $x_1=3$ times\n",
        "* word \"velocity\" $x_2=2$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0QmC1-e2vfg",
        "colab_type": "text"
      },
      "source": [
        "##Solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c30hNdDFz8Vm",
        "colab_type": "text"
      },
      "source": [
        "We have $K=3$ features ($3$ words in the vocabulary) and $n=3+2+0=5$ \"trials\" when we pick words from the \"bag\" with replacement.\n",
        "\n",
        "\n",
        "$$\n",
        "p_i=\\frac{n_i+ \\alpha}{N+K \\alpha}.\n",
        "$$\n",
        "\n",
        "So\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p_1&=\\frac{n_1+ \\alpha}{n+K \\alpha}=\\frac{3+ 0.1}{5+3 \\times 0.1}=0.58, \\\\\n",
        "p_2&=\\frac{n_2+ \\alpha}{n+K \\alpha}=\\frac{2+ 0.1}{5+3 \\times 0.1}=0.4, \\\\\n",
        "p_3&=\\frac{n_3+ \\alpha}{n+K \\alpha}=\\frac{0+ 0.1}{5+3 \\times 0.1}=0.02.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We see that\n",
        "\n",
        "$$\n",
        "p_1+p_2+p_3=0.58+0.4+0.02=1.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuCoZfNIXBw",
        "colab_type": "text"
      },
      "source": [
        "##By hand multivariate Bernoulli model parameters estimation for $1$ tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp6UKd7w38PH",
        "colab_type": "text"
      },
      "source": [
        "In the previos setup adopt multivariate Bernoulli model. \n",
        "Now we need to count not total number of occurencies in all documents but rather number of documents in which the word occurs.\n",
        "\n",
        "Your 1st document contains\n",
        "\n",
        "* word \"position\" is absent (bool frequency is $0$)\n",
        "* word \"velocity\" is present (bool frequency is $1$)\n",
        "* word \"stocks\" is absent (bool frequency is $0$)\n",
        "\n",
        "Your 2nd document contains\n",
        "\n",
        "* word \"position\" is absent (bool frequency is $0$)\n",
        "* word \"velocity\" is present (bool frequency is $1$)\n",
        "* word \"stocks\" is absent (bool frequency is $0$)\n",
        "\n",
        "Your 3rd document contains\n",
        "\n",
        "* word \"position\" is present (bool frequency is $1$)\n",
        "* word \"velocity\" is absent (bool frequency is $0$)\n",
        "* word \"stocks\" is absent (bool frequency is $0$)\n",
        "\n",
        "We can add boolean frequencies\n",
        "* word \"position\" is present in $1$ documents\n",
        "* word \"velocity\" is present in $2$ documents\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "The model \n",
        "\n",
        "$$\n",
        "P(\\{x_1,x_2,x_3\\})= p_{1}^{x_1}(1-p_1)^{1-x_1} \\times p_{2}^{x_2}(1-p_2)^{1-x_2} \\times p_{3}^{x_3}(1-p_3)^{1-x_3}\n",
        "$$\n",
        "\n",
        "where $x_i$ is either $0$ or $1$ (i.e. the word is either present in the document or not).\n",
        "\n",
        "With given data use emphirical frequency (with smoothing $\\alpha=0.1$) to estimate model parameters $p_1,p_2,p_3$ and verify that $p_1+p_2+p_3=1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98bLnOnwMr9X",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_jzP0KJMqQm",
        "colab_type": "text"
      },
      "source": [
        "$K=3$ (three words in the dictionary) and $n=1+2+0=3$. Therefore\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p_1&=\\frac{n_1+ \\alpha}{n+K \\alpha}=\\frac{1+ 0.1}{3+3 \\times 0.1}=0.33, \\\\\n",
        "p_2&=\\frac{n_2+ \\alpha}{n+K \\alpha}=\\frac{2+ 0.1}{3+3 \\times 0.1}=0.64, \\\\\n",
        "p_3&=\\frac{n_3+ \\alpha}{n+K \\alpha}=\\frac{0+ 0.1}{3+3 \\times 0.1}=0.03.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOp6EfyFJphl",
        "colab_type": "text"
      },
      "source": [
        "#Automatic parameters estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WkxuWa7JuZD",
        "colab_type": "text"
      },
      "source": [
        "Write class that performs multinomial and multivariate parameters estimation. The class gets array with counts from Vectorizer and returns vector with $p_i$s. You can also set $\\alpha$ in the constructor (default value is $1$). Verify your code with the example above.\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb_2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebBw1UgofzlR",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDElGnbOaCx1",
        "colab_type": "code",
        "outputId": "3b8ff3ee-8831-4885-d394-7b03bbbebf23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "class MultinomialParametersEstimator:\n",
        "  \"\"\"Calculates parameters for Multinomial/multivariate Bernoulli model\"\"\"\n",
        "  def __init__(self, alpha=1):\n",
        "    self.alpha = alpha\n",
        "    \n",
        "  def get_params_for_tag(self, counts):\n",
        "    sum = counts[0]\n",
        "    K = len(sum)\n",
        "    for i in range(1,len(counts)):\n",
        "      for j in range(0,K):\n",
        "        sum[j] += counts[i][j]\n",
        "    \n",
        "    n = 0\n",
        "    for i in range(0,K):\n",
        "      n += sum[i]\n",
        "      \n",
        "    for i in range(0,K):\n",
        "      sum[i] = (sum[i] + self.alpha) / (n + K * self.alpha)\n",
        "    return sum\n",
        "\n",
        "  def get_params(self, counts, tags):\n",
        "    dict={}\n",
        "    out=[]\n",
        "    j=0\n",
        "    for i in tags:\n",
        "      if not i in dict:\n",
        "        dict[i]=[] \n",
        "      dict[i].append(counts[j])\n",
        "      j += 1\n",
        "    for i in dict:\n",
        "      out.append(self.get_params_for_tag(dict[i]))\n",
        "    return out\n",
        "\n",
        "counts=[[1,2,3],[3,4,5],[3,4,5]]\n",
        "tags=[1,1,0]\n",
        "e=MultinomialParametersEstimator()\n",
        "e.get_params(counts,tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.23809523809523808, 0.3333333333333333, 0.42857142857142855],\n",
              " [0.26666666666666666, 0.3333333333333333, 0.4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO1iam_mdLTV",
        "colab_type": "code",
        "outputId": "b38eb477-0ad4-4d5f-f6ca-005df6b95e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "counts = [[0,1,0],[0,1,0],[3,0,0]]\n",
        "m = MultinomialParametersEstimator(0.1)\n",
        "m.parameter(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5849056603773585, 0.39622641509433965, 0.01886792452830189]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZkN4wgDgEUt",
        "colab_type": "text"
      },
      "source": [
        "#Calculating likelihoods in multinomial model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRQ5iYJzh2B-",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "L(\\{p_1,p_2,p_3\\}|\\{x_1,x_2,x_3\\})=P(\\{x_1,x_2,x_3\\}|\\{p_1,p_2,p_3\\})=\\frac{(x_1+x_2+x_3)!}{x_1!x_2!x_3!} p_{1}^{x_1}p_{2}^{x_2}p_{3}^{x_3}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkZT-Twjm4bp",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk6WaKObgKU5",
        "colab_type": "code",
        "outputId": "7078a346-4d4f-44a0-975a-ced4bed8d102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math \n",
        "\n",
        "class multinomialLikelihoodCalculator:\n",
        "  def likelihood(self, x, p):\n",
        "    sum = 0\n",
        "    \n",
        "    for i in range (0, len(x)):\n",
        "      sum += x[i]\n",
        "\n",
        "    P = math.gamma(sum+1)\n",
        " \n",
        "    for i in range(0, len(x)):\n",
        "      P *= (p[i])**(x[i])/math.gamma(x[i] + 1)\n",
        "      #print(\"x\",x[i],\"p\",p[i], \"P\",P)\n",
        "    \n",
        "    return P\n",
        "\n",
        "  def calculateLikelihoods(self, xx, pp):\n",
        "    out = []\n",
        "    for i in range(0, len(pp)):\n",
        "      out.append(self.likelihood(xx, pp[i]))\n",
        "    \n",
        "    return out\n",
        "\n",
        "  \n",
        "LC = multinomialLikelihoodCalculator()\n",
        "LC.likelihood([1,2],[3,4])\n",
        "LC.calculateLikelihoods([2,4,5],[[0.33,0.64,0.03],[0.2,0.3,0.4]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.076715106533376e-06, 0.022992076800000004]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UB-8c_7iLT0",
        "colab_type": "text"
      },
      "source": [
        "#Posterior calculator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_GNHH1yiOXY",
        "colab_type": "text"
      },
      "source": [
        "Given arrays of Likelihoods and prior probabilities we multiply them elementwise and get posterior probabilities.\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb_4.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvKQe1xPlfmV",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNV-qDI94NP3",
        "colab_type": "code",
        "outputId": "fa655f09-9858-4c6d-bb9e-9fd07dc4a520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "class posteriorCalculator:\n",
        "  def calculatePosteriors(self, L, p):\n",
        "    out=[]\n",
        "    for i in range(0, len(L)):\n",
        "      out.append(p[i] * L[i])\n",
        "    return out\n",
        "\n",
        "TE = posteriorCalculator()\n",
        "TE.calculatePosteriors([0.6, 0.4],[0.3, 0.7])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18, 0.27999999999999997]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXMpDBZ5iS3e",
        "colab_type": "text"
      },
      "source": [
        "#Decision Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_F56TCaiYja",
        "colab_type": "text"
      },
      "source": [
        "We will also need code that returns index of the maximum element in an array.\n",
        "\n",
        "![](https://raw.githubusercontent.com/fbeilstein/machine_learning/master/lecture_5_naive_bayes/nb_5.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l09rwMMSnEkk",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAtt0wMvnHS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class decisionRule:\n",
        "  def getArgMax(self, posterior):\n",
        "    return max(range(len(posterior)), key=posterior.__getitem__)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hpEfP40JhLR",
        "colab_type": "text"
      },
      "source": [
        "#Text classification with our own corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExPn4o4Klu7q",
        "colab_type": "text"
      },
      "source": [
        "Now we can test our code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8NIG--8lVju",
        "colab_type": "code",
        "outputId": "9cd29364-10a8-40e0-92b4-a3bbf6edd71d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "categories=[\"Physics\", \"Economy\", \"Religion\"]\n",
        "\n",
        "P=[\"In Newtonian mechanics, linear momentum, translational momentum, or simply momentum (pl. momenta) is the product of the mass and velocity of an object.\",\n",
        "   \"A neutrino (denoted by the Greek letter ν) is a fermion (an elementary particle with half-integer spin) that interacts only via the weak subatomic force and gravity.\",\n",
        "   \"In physics, the center of mass of a distribution of mass in space is the unique point where the weighted relative position of the distributed mass sums to zero. This is the point to which a force may be applied to cause a linear acceleration without an angular acceleration.\"]\n",
        "\n",
        "E=[\"Money is any item or verifiable record that is generally accepted as payment for goods and services and repayment of debts, such as taxes, in a particular country or socio-economic context.\",\n",
        "   \"Originally money was a form of receipt, representing grain stored in temple granaries in Sumer in ancient Mesopotamia and later in Ancient Egypt. In this first stage of currency, metals were used as symbols to represent value stored in the form of commodities. \",\n",
        "   \"In economics, inflation is a increase in the general price level of goods and services in an economy over a period of time. When the general price level rises, each unit of currency buys fewer goods and services; consequently, inflation reflects a reduction in the purchasing power per unit of money – a loss of real value in the medium of exchange and unit of account within the economy.\"]\n",
        "\n",
        "R=[\"Christianity is an Abrahamic monotheistic religion based on the life and teachings of Jesus of Nazareth. Its adherents, known as Christians, believe that Jesus is the Christ, the Son of God, and the savior of all people, whose coming as the Messiah was prophesied in the Hebrew Bible, called the Old Testament in Christianity, and chronicled in the New Testament.\",\n",
        "   \"Traditionalist Catholicism is a set of religious beliefs made up of the customs, traditions, liturgical forms, public, private and group devotions, and presentations of the teaching of the Catholic Church before the Second Vatican Council\",\n",
        "   \"Most modern scholars believe that John the Baptist performed a baptism on Jesus, and view it as a historical event to which a high degree of certainty can be assigned.\"]\n",
        "\n",
        "tags = [0,0,0,1,1,1,2,2,2]\n",
        "corpus = P + E + R\n",
        "\n",
        "d=\"Physics is economics money force and gravity\"\n",
        "\n",
        "TE = tagsEstimator()\n",
        "tagsPriorProb = TE.calculatePriors(tags)\n",
        "\n",
        "\n",
        "SCV = SimpleCountVectorizer(binary=False)\n",
        "SCV.fit_transform(corpus)\n",
        "arrayOfCounts = SCV.toarray()\n",
        "vocabulary =   SCV.get_feature_names()\n",
        "\n",
        "WC = docWordCounter()\n",
        "countOfDocument = WC.count(vocabulary, d)\n",
        "MPE = MultinomialParametersEstimator(0.1)\n",
        "estimatedParams = MPE.get_params(arrayOfCounts, tags)\n",
        "LC = multinomialLikelihoodCalculator()\n",
        "likelihood = LC.calculateLikelihoods(countOfDocument, estimatedParams)\n",
        "PC = posteriorCalculator()\n",
        "posterior = PC.calculatePosteriors(likelihood, tagsPriorProb)\n",
        "DR = decisionRule()\n",
        "index_max = DR.argMax(posterior)\n",
        "print(\"Answer:\", categories[index_max])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: Physics\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}