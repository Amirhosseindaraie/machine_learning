{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_4.5_statistics.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fbeilstein/machine_learning/blob/master/seminar_5_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD6O72hfJaYj",
        "colab_type": "text"
      },
      "source": [
        "#Estimating $P(C_k)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUgepi7XIr-T",
        "colab_type": "text"
      },
      "source": [
        "Suppose in a training set there are 15 documents labeled $C_1=\"Physics\"$, 25 documents labeled $C_2=\"Economics\"$ and $C_3=\"Religion\"$.\n",
        "\n",
        "Estimate $P(C_k)$ using MLE (emphirical frequencies)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RToFZMcynz4",
        "colab_type": "text"
      },
      "source": [
        "#Multinomial model parameters estimation\n",
        "\n",
        "Suppose your dictionary contains words $\\{\"position\", \"velocity\", \"stocks\"\\}$.\n",
        "Suppose you have $3$ documents labeled as \"Physics\".\n",
        "\n",
        "Your 1st document contains \n",
        "\n",
        "* word \"position\" $x_1=0$ times\n",
        "* word \"velocity\" $x_2=1$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "Your 2nd document contains \n",
        "\n",
        "* word \"position\" $x_1=0$ times\n",
        "* word \"velocity\" $x_2=1$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "Your 3rd document contains \n",
        "\n",
        "* word \"position\" $x_1=3$ times\n",
        "* word \"velocity\" $x_2=0$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "Suppose you adopt multinomial model for word occurence\n",
        "\n",
        "$$\n",
        "P(\\{x_1,x_2,x_3\\}|\"Phycics\")=\\frac{(x_1+x_2+x_3)!}{x_1!x_2!x_3!} p_{1}^{x_1}p_{2}^{x_2}p_{3}^{x_3}\n",
        "$$\n",
        "\n",
        "With given data use emphirical frequency (with smoothing $\\alpha=0.1$) to estimate model parameters $p_1,p_2,p_3$ and verify that $p_1+p_2+p_3=1$.\n",
        "\n",
        "Note: aggregate our three documents to get \n",
        "\n",
        "* word \"position\" $x_1=3$ times\n",
        "* word \"velocity\" $x_2=2$ times\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0QmC1-e2vfg",
        "colab_type": "text"
      },
      "source": [
        "##Solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c30hNdDFz8Vm",
        "colab_type": "text"
      },
      "source": [
        "We have $K=3$ features ($3$ words in the vocabulary) and $n=5$ trials.\n",
        "\n",
        "\n",
        "$$\n",
        "p_i=\\frac{n_i+ \\alpha}{N+K \\alpha}.\n",
        "$$\n",
        "\n",
        "So\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p_1&=\\frac{n_1+ \\alpha}{n+K \\alpha}=\\frac{3+ 0.1}{5+3 \\times 0.1}=0.58, \\\\\n",
        "p_2&=\\frac{n_2+ \\alpha}{n+K \\alpha}=\\frac{2+ 0.1}{5+3 \\times 0.1}=0.4, \\\\\n",
        "p_3&=\\frac{n_3+ \\alpha}{n+K \\alpha}=\\frac{0+ 0.1}{5+3 \\times 0.1}=0.02.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We see that\n",
        "\n",
        "$$\n",
        "p_1+p_2+p_3=0.58+0.4+0.02=1.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSuCoZfNIXBw",
        "colab_type": "text"
      },
      "source": [
        "#Multivariate Bernoulli model parameters estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp6UKd7w38PH",
        "colab_type": "text"
      },
      "source": [
        "In the previos setup adopt multivariate Bernoulli model. \n",
        "Now we need to count not total number of occurencies in all documents but rather number of documents in which the word occurs.\n",
        "\n",
        "* word \"position\" is present in $2$ documents\n",
        "* word \"velocity\" is present in $2$ documents\n",
        "* word \"stocks\" $x_3=0$ times.\n",
        "\n",
        "The model \n",
        "\n",
        "$$\n",
        "P(\\{x_1,x_2,x_3\\})= p_{1}^{x_1}(1-p_1)^{1-x_1} \\times p_{2}^{x_2}(1-p_2)^{1-x_2} \\times p_{3}^{x_3}(1-p_3)^{1-x_3}\n",
        "$$\n",
        "\n",
        "where $x_i$ is either $0$ or $1$ (i.e. the word is either present in the document or not).\n",
        "\n",
        "With given data use emphirical frequency (with smoothing $\\alpha=0.1$) to estimate model parameters $p_1,p_2,p_3$ and verify that $p_1+p_2+p_3=1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98bLnOnwMr9X",
        "colab_type": "text"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_jzP0KJMqQm",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "p_1&=\\frac{n_1+ \\alpha}{n+K \\alpha}=\\frac{2+ 0.1}{4+3 \\times 0.1}=0.49, \\\\\n",
        "p_2&=\\frac{n_2+ \\alpha}{n+K \\alpha}=\\frac{2+ 0.1}{4+3 \\times 0.1}=0.49, \\\\\n",
        "p_3&=\\frac{n_3+ \\alpha}{n+K \\alpha}=\\frac{0+ 0.1}{4+3 \\times 0.1}=0.02.\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    }
  ]
}