* Статистика

- Назвати кілька функцій статистики.
- Пояснити поняття генеральної сукупності та вибірки.
- Що таке репрезентативна та нерепрезентативна вибірки?
- Які бувають типи даних та які операції можна з ними виконувати?
- Чи відрізняється випадкова змінна від своєї реалізації?
- Пояснити (написати формулу де можливо): середнє значення, мода, медіана, варіація, стандартне відхилення.
- Чому в оцінках часто віддають перевагу медіані перед середнім?
- Що таке інтерквартильна відстань?
- Назвати кілька випадкових розподілів, навести приклад де вони виникають.
- Що таке IID?
- Що таке естіматор? Назвіть кілька естіматорів.
- [складна] Що таке biased (uncorrected) та unbiased (corrected) варіації? Чим відрізняються та в чім причина різниці?
- [складна] Назвіть принаймні один естіматор до кожного випадку: не потребує корекції (завжди unbiased), може бути скорегований (буває biased (uncorrected) та unbiased (corrected)), не може бути скорегований для довільного випадкового розподілу.
- Що таке MLE? Як це працює?
- [складна] Виконати MLE для нормального розподілу з зафіксованою варіацією.
- Що таке коваріація та кореляція (формули)?
- Чи означає кореляція причинно-наслідковий зв'язок?
- Що таке парадокс Сімпсона?
- Що означає "всі моделі неправильні, але деякі корисні"?

* Наївний Баєс

- Вивести формулу Баєса.
- [складна] Розв'язати задачу: в двох мішках знаходяться 1 бусина і 1 перлина (але де що не відомо). Я випадковим чином вибираю мішок, кладу туди перлину, перемішую вміст і наугад витягаю. Витягнутою виявляється перлина. З якою імовірністю в цьому мішку залишилась бусина?
- Опишіть класифікацію спам-не спам за 1 ключовим словом баєсівським класифікатором.
- Що означає "наївний баєсів класифікатор"?
- Що означає, що баєсів клафікатор гаусівський, мультиномінальний чи бернулів?
- Баєсів класифікатор для класифікації текстів.

* Оптимізація

- Що таке опукла функція
- [складна] Дуальна задача. Слабка і сильна дуальність.
- Тестові функції. Навіть принаймні кілька "патологій".
- Класифікація методів оптимізації. Назвіть хоча б по одному методу кожного порядку.
- Метод довірчих регіонів.

* Регресія

- Що мінімізується в методі лінійної регресії?
- [складна] Регресія з точки зору статистики (вивести компактну формулу).
- зробити регресію для точок {(1,1),(2,4),(4,5)}
- [складна] Регресія з точки зору лінійної алгебри.
- Регресія в довільному базисі.
- Регуляризації Ріджа, Лассо, еластік нет.

* Машини опорних векторів

- Що мінімізується чи максимізується в SVM? намалювати картинку для лінійно роздільних класів та непроникною границею.
- Як пом'якшують границі SVM?
- Навіщо переходити до дуальної задачі? Назвіть принаймні 2 ядра.
- Що таке опорний вектор?
- [складна] Чому важливо масштабувати фічі для SVM?
- [складна] Чи можна отримати від SVM імовірність правильної класифікації? А якусь міру впевненості?
- [складна] Ви натренували SVM з ядром RBF і отримали андерфіт. Чи є сенс змінювати гамма та С? В більшу чи меншу сторону?

* Дерева рішень та випадковий ліс

- Яка очікувана висота дерева рішень, що побудоване на 1 мільйоні датапойнтів (жодні додаткові обмеження при побудові не використовуються)?
- Чи є індекс Джині даного вузла зазвичай більший чи менший за індекс батьківського вузла? Зазвичай чи завжди більший чи менший?
- Якщо дерево рішень перенавчилося, то чи є смисл зменшувати параметр max_depth? 
- Якщо дерево рішень недонавчилося, то чи є смисл масштабувати ознаки?

- Ви натренували 5 різних моделей на одних і тих самих даних і отримали на кожній з них 95% точності. Чи можна очікувати, що об'єднання цих моделей дасть кращий результат? Якщо так, то в яких випадках? Якщо ні, то чому?
- В чім полягає різниця між жорстким та м'яким класифікатором з голосуванням (hard and soft voting classifiers)?
- Чи можна прискорити бутстрап агрегацію (bagging ensemble) використовуючи розподілені обчислення на багатьох серверах? А як щодо бустінгу та випадкового лісу?
- В чому перевага оцінки out-of-bag?
- Що робить Extra-Trees більш випадковими за випадковий ліс? Як це може допомогти в класифікації? Чи є побудова такого дерева швидшою за побудову звичайного дерева рішень?
- Які гіперпараметри та як варто змінювати, якщо ансамбль AdaBoost проявляє недонавчання?

* Метод головних компонент

- Для чого використовується пониження розмірності датасету? В чому полягають основні недоліки?
- Що таке прокляття розмірності?
- Чи можна інвертувати операцію пониження розмірності? Якщо так, то як? Якщо ні, то чому?
- Чи можна використовувати метод головних компонент для пониження розмірності сильно нелінійного датасету?
- Допустимо, Ви використовуєте метод головних компонент на даних розмірністю 1000 і хочете зберегти 95% поясненої варіації. Яку розмірність вихідного датасету Ви можете очікувати?
- В яких випадках варто використовувати наступні варіації методу головних компонент: простий (vanilla PCA), інкрементальний (Incremental PCA), рандомізований (Randomized PCA) та ядерний (Kernel PCA)?
- Як оцінити якість алгоритму пониження розмірності на заданому датасеті?
- Чи є сенс у послідовному використанні двох різних алгоритмів пониження розмірності?



-- Decision Trees Solutions --
1. The depth of a well-balanced binary tree containing m leaves is equal to log 2 (m) ,
rounded up. A binary Decision Tree (one that makes only binary decisions, as is
the case of all trees in Scikit-Learn) will end up more or less well balanced at the
end of training, with one leaf per training instance if it is trained without restric‐
tions. Thus, if the training set contains one million instances, the Decision Tree
will have a depth of log 2 (10 6 ) ≈ 20 (actually a bit more since the tree will generally
not be perfectly well balanced).
2. A node’s Gini impurity is generally lower than its parent’s. This is ensured by the
CART training algorithm’s cost function, which splits each node in a way that
minimizes the weighted sum of its children’s Gini impurities. However, if one
child is smaller than the other, it is possible for it to have a higher Gini impurity
than its parent, as long as this increase is more than compensated for by a
decrease of the other child’s impurity. For example, consider a node containing
1 2
4 2
four instances of class A and 1 of class B. Its Gini impurity is 1 − 5 − 5 = 0.32.
Now suppose the dataset is one-dimensional and the instances are lined up in the
following order: A, B, A, A, A. You can verify that the algorithm will split this
node after the second instance, producing one child node with instances A, B,
and the other child node with instances A, A, A. The first child node’s Gini
1 2
1 2
impurity is 1 − 2 − 2 = 0.5, which is higher than its parent. This is compensated
for by the fact that the other node is pure, so the overall weighted Gini impurity
2
3
is 5 × 0.5 + 5 × 0 = 0.2 , which is lower than the parent’s Gini impurity.
3. If a Decision Tree is overfitting the training set, it may be a good idea to decrease
max_depth , since this will constrain the model, regularizing it.
4. Decision Trees don’t care whether or not the training data is scaled or centered;
that’s one of the nice things about them. So if a Decision Tree underfits the train‐
ing set, scaling the input features will just be a waste of time.

-- Random Forests Solutions --

1. If you have trained five different models and they all achieve 95% precision, you
can try combining them into a voting ensemble, which will often give you even
better results. It works better if the models are very different (e.g., an SVM classi‐
fier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is
even better if they are trained on different training instances (that’s the whole
point of bagging and pasting ensembles), but if not it will still work as long as the
models are very different.
2. A hard voting classifier just counts the votes of each classifier in the ensemble
and picks the class that gets the most votes. A soft voting classifier computes the
average estimated class probability for each class and picks the class with the
highest probability. This gives high-confidence votes more weight and often per‐
forms better, but it works only if every classifier is able to estimate class probabil‐
ities (e.g., for the SVM classifiers in Scikit-Learn you must set
probability=True ).
3. It is quite possible to speed up training of a bagging ensemble by distributing it
across multiple servers, since each predictor in the ensemble is independent of
the others. The same goes for pasting ensembles and Random Forests, for the
same reason. However, each predictor in a boosting ensemble is built based on
the previous predictor, so training is necessarily sequential, and you will not gain
anything by distributing training across multiple servers. Regarding stacking
ensembles, all the predictors in a given layer are independent of each other, so
they can be trained in parallel on multiple servers. However, the predictors in one
layer can only be trained after the predictors in the previous layer have all been
trained.
4. With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated
using instances that it was not trained on (they were held out). This makes it pos‐
sible to have a fairly unbiased evaluation of the ensemble without the need for an
additional validation set. Thus, you have more instances available for training,
and your ensemble can perform slightly better.
5. When you are growing a tree in a Random Forest, only a random subset of the
features is considered for splitting at each node. This is true as well for Extra-
Trees, but they go one step further: rather than searching for the best possible
thresholds, like regular Decision Trees do, they use random thresholds for each
feature. This extra randomness acts like a form of regularization: if a Random
Forest overfits the training data, Extra-Trees might perform better. Moreover,
since Extra-Trees don’t search for the best possible thresholds, they are much
faster to train than Random Forests. However, they are neither faster nor slower
than Random Forests when making predictions.
6. If your AdaBoost ensemble underfits the training data, you can try increasing the
number of estimators or reducing the regularization hyperparameters of the base
estimator. You may also try slightly increasing the learning rate.
7. If your Gradient Boosting ensemble overfits the training set, you should try
decreasing the learning rate. You could also use early stopping to find the right
number of predictors (you probably have too many).

-- PCA Solutions --

1. Motivations and drawbacks:
• The main motivations for dimensionality reduction are:
— To speed up a subsequent training algorithm (in some cases it may even
remove noise and redundant features, making the training algorithm per‐
form better).
— To visualize the data and gain insights on the most important features.
— Simply to save space (compression).
• The main drawbacks are:
— Some information is lost, possibly degrading the performance of subse‐
quent training algorithms.
— It can be computationally intensive.
— It adds some complexity to your Machine Learning pipelines.
— Transformed features are often hard to interpret.
2. The curse of dimensionality refers to the fact that many problems that do not
exist in low-dimensional space arise in high-dimensional space. In Machine
Learning, one common manifestation is the fact that randomly sampled high-
dimensional vectors are generally very sparse, increasing the risk of overfitting
and making it very difficult to identify patterns in the data without having plenty
of training data.
3. Once a dataset’s dimensionality has been reduced using one of the algorithms we
discussed, it is almost always impossible to perfectly reverse the operation,
because some information gets lost during dimensionality reduction. Moreover,
while some algorithms (such as PCA) have a simple reverse transformation pro‐
cedure that can reconstruct a dataset relatively similar to the original, other algo‐
rithms (such as T-SNE) do not.
4. PCA can be used to significantly reduce the dimensionality of most datasets, even
if they are highly nonlinear, because it can at least get rid of useless dimensions.
However, if there are no useless dimensions—for example, the Swiss roll—then
reducing dimensionality with PCA will lose too much information. You want to
unroll the Swiss roll, not squash it.
5. That’s a trick question: it depends on the dataset. Let’s look at two extreme exam‐
ples. First, suppose the dataset is composed of points that are almost perfectly
aligned. In this case, PCA can reduce the dataset down to just one dimension
while still preserving 95% of the variance. Now imagine that the dataset is com‐
posed of perfectly random points, scattered all around the 1,000 dimensions. In
this case all 1,000 dimensions are required to preserve 95% of the variance. So the
answer is, it depends on the dataset, and it could be any number between 1 and
1,000. Plotting the explained variance as a function of the number of dimensions
is one way to get a rough idea of the dataset’s intrinsic dimensionality.
6. Regular PCA is the default, but it works only if the dataset fits in memory. Incre‐
mental PCA is useful for large datasets that don’t fit in memory, but it is slower
than regular PCA, so if the dataset fits in memory you should prefer regular
PCA. Incremental PCA is also useful for online tasks, when you need to apply
PCA on the fly, every time a new instance arrives. Randomized PCA is useful
when you want to considerably reduce dimensionality and the dataset fits in
memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is
useful for nonlinear datasets.
7. Intuitively, a dimensionality reduction algorithm performs well if it eliminates a
lot of dimensions from the dataset without losing too much information. One
way to measure this is to apply the reverse transformation and measure the
reconstruction error. However, not all dimensionality reduction algorithms pro‐
vide a reverse transformation. Alternatively, if you are using dimensionality
reduction as a preprocessing step before another Machine Learning algorithm
(e.g., a Random Forest classifier), then you can simply measure the performance
of that second algorithm; if dimensionality reduction did not lose too much
information, then the algorithm should perform just as well as when using the
original dataset.
8. It can absolutely make sense to chain two different dimensionality reduction
algorithms. A common example is using PCA to quickly get rid of a large num‐
ber of useless dimensions, then applying another much slower dimensionality
reduction algorithm, such as LLE. This two-step approach will likely yield the
same performance as using LLE only, but in a fraction of the time.
